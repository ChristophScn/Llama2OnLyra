{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is inspired by this [blog post](https://huggingface.co/blog/llama2).  \n",
    "\n",
    "The author of this notebook is [Christoph Schnell](mailto:christoph@schnell.de) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Perform Inference with Llama 2 Models on Lyra\n",
    "To use Llama 2 on Lyra with Python you can use the [transformers](https://huggingface.co/docs/transformers/index) library from Hugging Face."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply for access\n",
    "To run Llama 2 with the transformers library, you will first need to request access to Meta's models (1) and Meta's Hugging Face repositories (2).\n",
    "\n",
    "Start with requesting access to Meta's models [here](https://ai.meta.com/resources/models-and-libraries/llama-downloads/). You will *not* need to download the models manually once Meta has granted you access.\n",
    "\n",
    "Once Meta has granted you access, you will need to request for access to Meta's Hugging Face repositories. To request access, visit the repository of the Llama model you want to access (e.g. this [repository](https://huggingface.co/meta-llama/Llama-2-7b-hf)) and use the \"Access Llama 2 on Hugging Face\" form. More information can be found [here](https://huggingface.co/meta-llama).\n",
    "\n",
    "*Note: Your Hugging Face account email address MUST match the email address you provided on the Meta website, or your request will not be approved.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "Once you have successfully applied for access to the Hugging Face repositories you can start installing the necessary dependencies. You will need to install the transformers library from Hugging Face via pip and login to your Hugging Face account. You can do this by running the following commands in your terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "source": [
    "```bash\n",
    "pip install transformers\n",
    "huggingface-cli login\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference of the Llama 2 model\n",
    "Once you have installed the dependencies and logged into your Hugging Face account you can run inference of the Llama 2 model. On Lyra you can run all model sizes (7B, 13B, 70B). To load a model, run the following code:\n",
    "\n",
    "*Note that the first load for each model will take a while as the model is automatically downloaded*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "# Import modules and load model\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate text with the model you can use the pipeline you just created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Generate text\n",
    "sequences = pipeline(\n",
    "    '[INST]Is an apple a fruit? Start with yes or no and then explain your answer.[/INST]\\n\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate only the next line instead of an entire response, you can add the newline token as an end-of-sequence token to the text generation pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pipeline(\n",
    "    '[INST]Is an apple a fruit? Start with yes or no and then explain your answer.[/INST]\\n\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=[tokenizer.eos_token_id, tokenizer.encode(\"\\n\", add_special_tokens=False)[-1]],\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(\"------- Result -------\")\n",
    "    print(f\"{seq['generated_text']}\")\n",
    "    print(\"----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get only the next token probability you can run the following code:\n",
    "\n",
    "*Note that a word may be represented by multiple tokens; therefore, not every word has a one-to-one correspondence with a singular token.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import defaultdict\n",
    "prompt = '[INST]Is an apple a fruit? Start with yes or no and then explain your answer.[/INST]\\n\\n'\n",
    "\n",
    "inputs = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0).to(pipeline.device)\n",
    "outputs = pipeline.model(inputs)\n",
    "probs = outputs[0][:, -1, :]\n",
    "probs = torch.softmax(probs, dim=-1)\n",
    "probs = probs.cpu().detach().numpy()[0]\n",
    "token_probs = defaultdict(float)\n",
    "for token, prob in enumerate(probs):\n",
    "    token_probs[tokenizer.decode(token)] += prob\n",
    "\n",
    "for token, prob in sorted(token_probs.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"{token:<10}{prob:.2%}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
